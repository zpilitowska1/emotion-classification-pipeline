{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae296dc9",
   "metadata": {},
   "source": [
    "# Machine Translation: Polish to English\n",
    "This notebook demonstrates automatic translation of Polish text to English using a pre-trained transformer model from Hugging Face.\n",
    "Overview\n",
    "\n",
    "## Model: Helsinki-NLP/opus-mt-pl-en (Marian MT)\n",
    "## Task: Translate Polish sentences from MasterChef transcript to English\n",
    "## Framework: Transformers library with PyTorch backend\n",
    "\n",
    "## Use Case\n",
    "This translation is part of a larger NLP pipeline for emotion classification, where we need English translations to complement Polish text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905f34a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "# !pip install transformers[torch]`\n",
    "# !pip install sacrebleu\n",
    "# !pip install evaluate\n",
    "# !pip install sacrebleu\n",
    "# !pip install accelerate -U\n",
    "# !pip install gradio \n",
    "# !pip install kaleido cohere  openai tiktoken typing-extensions==4.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e1498b",
   "metadata": {},
   "source": [
    "# Dataset Exploration\n",
    "Loading a parallel English-Polish dataset to understand the translation format.\n",
    "Dataset Source: Gregniuki/english-polish-idioms\n",
    "This step helps us understand the expected input/output structure for our translation task.\n",
    "\n",
    "https://huggingface.co/datasets/Gregniuki/english-polish-idioms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3680e70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': 'Hello, how are you?', 'pl': 'Cześć, jak się masz?'}}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Gregniuki/english-polish-idioms\")\n",
    "print(dataset['train'][0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1065bc07",
   "metadata": {},
   "source": [
    "# Model and Tokenizer Loading\n",
    "Loading the pre-trained Marian MT model for Polish to English translation.\n",
    "## Model Details:\n",
    "\n",
    "Architecture: Marian Neural Machine Translation\n",
    "Training: Opus parallel corpus (high-quality multilingual data)\n",
    "Direction: Polish → English (pl-en)\n",
    "Performance: State-of-the-art for this language pair\n",
    "\n",
    "## Why This Model:\n",
    "\n",
    "Specifically trained on Polish-English pairs\n",
    "Good performance on conversational text\n",
    "Reasonable size for inference speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51e738d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zosia\\AppData\\Local\\anaconda3\\envs\\block-a2\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-pl-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89811ca7",
   "metadata": {},
   "source": [
    "# Transcript Translation Pipeline\n",
    "## Process Overview:\n",
    "\n",
    "1. Load Polish MasterChef transcript from Excel file\n",
    "2. For each Polish sentence:\n",
    "\n",
    "Tokenize using Marian tokenizer\n",
    "Generate English translation using pre-trained model\n",
    "Decode output tokens back to text\n",
    "\n",
    "\n",
    "3. Add translations as new column\n",
    "4. Save enhanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58375c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>S</th>\n",
       "      <th>I</th>\n",
       "      <th>D</th>\n",
       "      <th>Translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To jest MasterChef.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>This is MasterChef.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Szansę na tytuł najlepszego kucharza w Polsce ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Only 12 people have a chance of being the best...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oto oni.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>There they are.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Jestem dinozaurem, który chce walczyć, który ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"I'm a dinosaur who wants to fight, who won't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ten program daje mi ogromną siłę.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>This program gives me great strength.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence    S    I    D  \\\n",
       "0                                To jest MasterChef.  0.0  0.0  0.0   \n",
       "1  Szansę na tytuł najlepszego kucharza w Polsce ...  0.0  0.0  0.0   \n",
       "2                                           Oto oni.  0.0  0.0  0.0   \n",
       "3  \"Jestem dinozaurem, który chce walczyć, który ...  0.0  0.0  0.0   \n",
       "4                  Ten program daje mi ogromną siłę.  0.0  0.0  0.0   \n",
       "\n",
       "                                         Translation  \n",
       "0                                This is MasterChef.  \n",
       "1  Only 12 people have a chance of being the best...  \n",
       "2                                    There they are.  \n",
       "3  \"I'm a dinosaur who wants to fight, who won't ...  \n",
       "4              This program gives me great strength.  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "df = pd.read_excel(r\"C:\\Users\\zosia\\Documents\\GitHub\\fae2-nlpr-group-group-17-1\\Task 7\\STT_Assembly.xlsx\") \n",
    "\n",
    "translations = []\n",
    "\n",
    "\n",
    "for sent in df['Sentence']:\n",
    "    inputs = tokenizer(sent, return_tensors = 'pt', truncation = True, padding = True)\n",
    "    outputs = model.generate(**inputs) #unpacking dictionary\n",
    "    translated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    translations.append(translated)\n",
    "\n",
    "df['Translation'] = translations\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f4ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['S', 'I','D'])\n",
    "df.to_excel(\"STT_Assembly_translated.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ebe3ea",
   "metadata": {},
   "source": [
    "## Summary of Translation Model Findings\n",
    "\n",
    "Our translation analysis shows mostly good results with two main types of mistakes.\n",
    "\n",
    "## Types of Mistakes:\n",
    "\n",
    "Speech-to-Text Errors: Most problems come from wrong words in the original Polish text, not from bad translation. The model translates the wrong Polish words correctly into wrong English words.\n",
    "\n",
    "Multiple Meaning Words: Some mistakes happen with Polish words that have different meanings. For example, \"kropka\" can mean \"dot\" or \"period.\" The model sometimes picks the wrong meaning for the situation.\n",
    "\n",
    "## Overall Quality: \n",
    "\n",
    "The grammar in translations is correct. The model handles Polish language rules well and makes proper English sentences.\n",
    "\n",
    "## Main Problems: \n",
    "\n",
    "Translation quality depends mostly on having correct Polish text to start with. The model also sometimes struggles to pick the right meaning when Polish words have multiple meanings. When given good Polish text, the translation works well.\n",
    " \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "block-a2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
